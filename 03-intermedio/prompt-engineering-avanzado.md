# üß† Prompt Engineering Avanzado

> **Dominando la comunicaci√≥n con IA.** Ya conoces lo b√°sico. Ahora vamos a t√©cnicas avanzadas que usan los profesionales: chain of thought, few-shot complex, optimizaci√≥n de tokens y prompts que resuelven problemas de arquitectura compleja.

## üéØ ¬øQu√© aprender√°s aqu√≠?

- ‚úÖ Chain of Thought y razonamiento estructurado
- ‚úÖ Few-shot learning complejo
- ‚úÖ Role prompting avanzado con contexto
- ‚úÖ Optimizaci√≥n de tokens y costos
- ‚úÖ Meta-prompting y self-correction
- ‚úÖ Prompt chaining para tareas complejas

## üß© **Chain of Thought (CoT) Avanzado**

### **CoT B√°sico vs Avanzado**

**‚ùå CoT B√°sico:**
```
Explica por qu√© mi app Android es lenta paso a paso.
```

**‚úÖ CoT Avanzado:**
```
Act√∫a como un senior Android performance engineer. Analiza este problema sistem√°ticamente:

CONTEXTO: App Android con 50k+ usuarios activos, lag notable en scrolling

INFORMACI√ìN DEL SISTEMA:
- Dispositivos afectados: Gama media (4GB RAM)
- Versi√≥n Android: 8.0 - 13.0
- Arquitectura: MVVM + Room + Retrofit
- S√≠ntomas: Stuttering en RecyclerView, ANRs ocasionales

AN√ÅLISIS REQUERIDO:
1. **Memory Analysis**: ¬øQu√© patrones de memory leaks podr√≠an causar esto?
2. **Threading Issues**: ¬øHay operaciones blocking en main thread?
3. **Database Performance**: ¬øLas queries de Room est√°n optimizadas?
4. **Network Layer**: ¬øRetrofit est√° configurado eficientemente?
5. **UI Rendering**: ¬øEl layout hierarchy es eficiente?

Para cada √°rea, proporciona:
- M√©todo de diagn√≥stico espec√≠fico
- Herramientas a usar (Profiler, Systrace, etc.)
- Soluci√≥n priorizada por impacto

C√ìDIGO SOSPECHOSO:
[incluir fragmentos relevantes aqu√≠]

Razona paso a paso, mostrando tu proceso de debugging.
```

### **CoT con verificaci√≥n auto-corregida**

```
Dise√±a la arquitectura de un sistema de chat en tiempo real.

PASO 1: An√°lisis de requisitos
- Lista los componentes principales
- Identifica patrones de comunicaci√≥n
- Estima la carga esperada

PASO 2: Dise√±o inicial
- Prop√≥n arquitectura high-level
- Selecciona tecnolog√≠as

PASO 3: Validaci√≥n cr√≠tica
- ¬øQu√© podr√≠a fallar en tu dise√±o?
- ¬øC√≥mo escala con 100k usuarios concurrentes?
- ¬øQu√© single points of failure existen?

PASO 4: Refinamiento
- Ajusta el dise√±o bas√°ndote en la validaci√≥n
- A√±ade redundancia y failover

PASO 5: Plan de implementaci√≥n
- Ordena desarrollo por prioridades
- Identifica MVPs y milestones

Muestra tu razonamiento en cada paso y c√≥mo cada decisi√≥n impacta las siguientes.
```

## üé≠ **Role Prompting Avanzado**

### **Multi-persona approach**

```
Necesito feedback sobre esta API design. Quiero perspectivas de diferentes roles:

API DESIGN:
[tu dise√±o aqu√≠]

PERSPECTIVA 1 - SENIOR BACKEND DEVELOPER:
Eval√∫a desde arquitectura t√©cnica, performance, escalabilidad.

PERSPECTIVA 2 - FRONTEND DEVELOPER:
¬øQu√© tan f√°cil es integrar esta API? ¬øFalta algo para el UI?

PERSPECTIVA 3 - DEVOPS ENGINEER:
Consideraciones de deployment, monitoring, troubleshooting.

PERSPECTIVA 4 - SECURITY EXPERT:
Vulnerabilidades potenciales, mejores pr√°cticas de seguridad.

PERSPECTIVA 5 - PRODUCT MANAGER:
¬øCumple con los requerimientos de negocio? ¬øEs mantenible?

Para cada perspectiva, proporciona:
- Principales concerns
- Sugerencias espec√≠ficas
- Deal-breakers si los hay
```

### **Role prompting con contexto hist√≥rico**

```
Act√∫a como un CTO con 15 a√±os de experiencia que ha visto m√∫ltiples ciclos de tecnolog√≠a.

SITUACI√ìN: Mi startup quiere migrar de monolito PHP a microservicios.

Tu experiencia incluye:
- Has visto fracasar migraciones prematuras
- Conoces los true costs de microservicios
- Entiendes trade-offs business vs technical
- Has manejado equipos de 5-50 developers

CONTEXTO DE LA EMPRESA:
- 10 developers
- 500k usuarios activos
- Revenue: $2M ARR
- Monolito actual funciona pero "no escala"

PREGUNTA: ¬øRecomendar√≠as esta migraci√≥n ahora?

Responde considerando:
- Timing y recursos
- Riesgos vs beneficios
- Alternativas menos disruptivas
- Lessons learned de tus experiencias anteriores

S√© brutalmente honesto sobre los pitfalls.
```

## üîÑ **Few-shot Learning Complejo**

### **Pattern recognition con evoluci√≥n**

```
Aprende este patr√≥n de refactoring y apl√≠calo:

EJEMPLO 1 - Python Legacy:
ANTES:
def get_user_data(user_id):
    conn = sqlite3.connect('db.sqlite')
    cursor = conn.cursor()
    cursor.execute(f"SELECT * FROM users WHERE id = {user_id}")
    result = cursor.fetchone()
    conn.close()
    return result

DESPU√âS:
@dataclass
class User:
    id: int
    name: str
    email: str

class UserRepository:
    def __init__(self, db_session: Session):
        self.db = db_session

    def get_by_id(self, user_id: int) -> Optional[User]:
        return self.db.query(User).filter(User.id == user_id).first()


PATRONES APLICADOS:
- SQL injection fix
- Connection management
- Type safety
- Separation of concerns
- Repository pattern

EJEMPLO 2 - Flutter State Management:
ANTES:
class CounterWidget extends StatefulWidget {
  @override
  _CounterWidgetState createState() => _CounterWidgetState();
}

class _CounterWidgetState extends State<CounterWidget> {
  int _counter = 0;

  void _increment() {
    setState(() {
      _counter++;
    });
    // HTTP call here
    http.post('/api/counter', body: {'count': _counter});
  }
}


DESPU√âS:
class CounterBloc extends Bloc<CounterEvent, CounterState> {
  final CounterRepository repository;

  CounterBloc(this.repository) : super(CounterInitial()) {
    on<IncrementPressed>(_onIncrement);
  }

  Future<void> _onIncrement(IncrementPressed event, Emitter<CounterState> emit) async {
    try {
      final newCount = state.count + 1;
      emit(CounterLoading());
      await repository.updateCount(newCount);
      emit(CounterSuccess(newCount));
    } catch (e) {
      emit(CounterError(e.toString()));
    }
  }
}


PATRONES APLICADOS:
- Business logic separation
- Error handling
- Testable architecture
- State management
- Repository pattern

AHORA APLICA ESTOS PATRONES A:
class ViewController: UIViewController {
    @IBOutlet weak var tableView: UITableView!
    var users: [User] = []

    override func viewDidLoad() {
        super.viewDidLoad()
        loadUsers()
    }

    func loadUsers() {
        let url = URL(string: "https://api.example.com/users")!
        URLSession.shared.dataTask(with: url) { data, response, error in
            guard let data = data else { return }
            self.users = try! JSONDecoder().decode([User].self, from: data)
            DispatchQueue.main.async {
                self.tableView.reloadData()
            }
        }.resume()
    }
}

Identifica los problemas y aplica los mismos patrones de refactoring.
```

## üéØ **Optimizaci√≥n de Tokens**

### **T√©cnicas de compresi√≥n de prompts**

**‚ùå Prompt verboso (450 tokens):**
```
I am a senior software developer working on a large-scale e-commerce application. We are currently using a microservices architecture with Node.js and Express for our backend services. Our database is PostgreSQL and we're using Redis for caching. We have been experiencing performance issues lately, particularly with our product search functionality. The search is becoming slow when users search for products, especially when they use multiple filters like price range, brand, category, and availability. Our current implementation queries the database directly for each search request and doesn't cache the results effectively. We need to optimize this system to handle increased traffic and provide faster search results. Can you help us design a better solution?
```

**‚úÖ Prompt optimizado (180 tokens):**
```
E-commerce app optimization needed:

STACK: Node.js/Express, PostgreSQL, Redis
PROBLEM: Slow product search with filters (price, brand, category, availability)
CURRENT: Direct DB queries, poor caching
GOAL: Handle high traffic, faster results

Design optimized search solution.
```

### **Token-aware prompt structure**

```python
def create_optimized_prompt(context, task, constraints):
    # Usar abreviaciones est√°ndar
    abbreviations = {
        "JavaScript": "JS",
        "TypeScript": "TS",
        "database": "DB",
        "application": "app",
        "function": "fn",
        "performance": "perf",
        "optimization": "opt"
    }

    # Estructura concisa pero completa
    prompt = f"""
    CONTEXT: {compress_text(context, abbreviations)}
    TASK: {task}
    CONSTRAINTS: {format_constraints(constraints)}

    Provide: code + brief explanation
    """

    return prompt
```

### **Costo por token awareness**

```python
def estimate_prompt_cost(prompt, model="gpt-4"):
    costs = {
        "gpt-4": {"input": 0.03, "output": 0.06},  # per 1K tokens
        "gpt-3.5-turbo": {"input": 0.001, "output": 0.002}
    }

    estimated_input_tokens = len(prompt.split()) * 1.3  # rough estimate
    estimated_output_tokens = 500  # estimate based on task

    total_cost = (
        (estimated_input_tokens / 1000) * costs[model]["input"] +
        (estimated_output_tokens / 1000) * costs[model]["output"]
    )

    return {
        "estimated_cost": total_cost,
        "input_tokens": estimated_input_tokens,
        "output_tokens": estimated_output_tokens
    }
```

## üîó **Prompt Chaining**

### **Multi-step architectural design**

**Paso 1: Requirements analysis**
```
Analiza estos requirements para una app de delivery:

USER STORIES:
- Usuario puede ordenar comida de restaurantes cercanos
- Restaurante puede gestionar men√∫ y √≥rdenes
- Delivery puede ver √≥rdenes asignadas y actualizar status
- Admin puede monitorear toda la operaci√≥n

CONSTRAINTS:
- 100k usuarios activos
- 1000 restaurantes
- 500 deliveries concurrentes
- Real-time updates cr√≠ticos

OUTPUT: Lista de functional y non-functional requirements estructurados.
NO dise√±es arquitectura a√∫n, solo an√°lisis.
```

**Paso 2: Architecture proposal**
```
Bas√°ndote en los requirements analizados anteriormente, dise√±a arquitectura:

INPUT: [Output del paso anterior]

DECISIONES A TOMAR:
- Monolito vs Microservicios
- Database strategy (SQL vs NoSQL vs Hybrid)
- Real-time communication (WebSockets, SSE, Push)
- Caching strategy
- File storage (images, etc.)

OUTPUT: High-level architecture diagram (text format) + technology decisions justificadas.
```

**Paso 3: Implementation roadmap**
```
Crea roadmap de implementaci√≥n para la arquitectura dise√±ada:

INPUT: [Architecture output del paso anterior]

PRIORIZACI√ìN:
- MVP features vs nice-to-have
- Technical risk assessment
- Team capacity (10 developers, 6 months)
- Dependencies entre componentes

OUTPUT:
- Sprint breakdown (2-week sprints)
- Critical path identification
- Risk mitigation strategies
- Testing strategy per component
```

## üß¨ **Meta-prompting**

### **Self-improving prompts**

```
Tu tarea es mejorar el siguiente prompt iterativamente:

PROMPT INICIAL:
"Crea una funci√≥n que ordene una lista"

CRITERIOS DE MEJORA:
1. Especificidad t√©cnica
2. Contexto de uso
3. Requerimientos de performance
4. Error handling expectations
5. Testing requirements

PROCESO:
1. Identifica qu√© falta en el prompt actual
2. Reescribe el prompt incorporando mejoras
3. Eval√∫a si el prompt mejorado obtendr√≠a mejores resultados
4. Si no, itera nuevamente

OBJETIVO: Prompt que genere c√≥digo production-ready.
```

### **Prompt debugging**

```
Este prompt no est√° generando los resultados esperados:

PROMPT PROBLEM√ÅTICO:
"Optimiza esta query SQL para que sea m√°s r√°pida"

RESULTADOS OBTENIDOS:
- Respuestas muy gen√©ricas
- No considera el schema espec√≠fico
- Sugerencias no aplicables

DIAGN√ìSTICO REQUERIDO:
1. ¬øQu√© informaci√≥n crucial falta?
2. ¬øC√≥mo deber√≠a estructurarse mejor?
3. ¬øQu√© contexto t√©cnico necesita?

REESCRIBE el prompt para obtener optimizaciones SQL espec√≠ficas y aplicables.
```

## üß™ **T√©cnicas experimentales avanzadas**

### **Constitutional AI approach**

```
Act√∫a como un code reviewer siguiendo estos principios constitucionales:

PRINCIPIOS BASE:
1. "Prefiere soluciones simples sobre complejas"
2. "La legibilidad es m√°s importante que la cleverness"
3. "Seguridad first, performance second, elegancia third"
4. "Si no puedes testear f√°cilmente, est√° mal dise√±ado"

C√ìDIGO A REVISAR:
[c√≥digo aqu√≠]

PROCESO:
1. Revisa aplicando cada principio
2. Si encuentras conflictos entre principios, explica el trade-off
3. Prioriza fixes bas√°ndote en la jerarqu√≠a de principios
4. Sugiere refactoring que alinee mejor con los principios

OBJETIVO: Code review que no solo encuentre bugs, sino que eleve la calidad arquitectural.
```

### **Debate-style prompting**

```
Tengo que decidir entre GraphQL y REST para mi nueva API.

SETUP: Simula un debate entre dos senior architects.

ARCHITECT A (Pro-GraphQL):
- Argumenta ventajas de GraphQL
- Usa casos reales de implementaci√≥n exitosa
- Aborda concerns t√≠picos sobre complexity

ARCHITECT B (Pro-REST):
- Defiende REST con argumentos s√≥lidos
- Menciona GraphQL pitfalls de proyectos reales
- Enfatiza simplicity y proven patterns

MODERADOR (t√∫):
- Hace preguntas dif√≠ciles a ambos
- Busca debilidades en cada argumento
- Eval√∫a fit para mi proyecto espec√≠fico

MI CONTEXTO:
- Team de 8 developers (mix junior/senior)
- API para mobile app + web dashboard
- Timeframe: 4 meses para MVP

FORMATO: Debate de 3 rondas + recomendaci√≥n final fundamentada.
```

## üìä **Measuring prompt effectiveness**

### **A/B testing your prompts**

```python
class PromptExperiment:
    def __init__(self, name):
        self.name = name
        self.variants = {}
        self.results = {}

    def add_variant(self, variant_name, prompt):
        self.variants[variant_name] = prompt

    def measure_effectiveness(self, variant, criteria):
        """
        Criteria:
        - accuracy: Does it solve the problem correctly?
        - completeness: Does it address all requirements?
        - efficiency: How concise is the response?
        - actionability: Can I immediately implement the solution?
        """
        scores = {}
        for criterion in criteria:
            scores[criterion] = self.evaluate_criterion(variant, criterion)

        return {
            "variant": variant,
            "scores": scores,
            "total_score": sum(scores.values()) / len(scores)
        }

# Ejemplo de uso:
experiment = PromptExperiment("code_review")

experiment.add_variant("basic",
    "Review this code and suggest improvements")

experiment.add_variant("structured",
    """Review this code systematically:

    1. Correctness: Does it work as intended?
    2. Performance: Any bottlenecks?
    3. Security: Vulnerabilities present?
    4. Maintainability: Is it readable and modular?

    Prioritize issues by severity.""")
```

### **Quality metrics for prompt outputs**

| Metric | Description | How to measure |
|--------|-------------|----------------|
| **Relevance** | Addresses the specific problem | 1-10 scale, manual review |
| **Completeness** | Covers all requirements | Checklist completion % |
| **Accuracy** | Technical correctness | Code compiles/tests pass |
| **Actionability** | Can implement immediately | Binary: yes/no |
| **Efficiency** | Concise but complete | Response length vs value |

## üöÄ **Advanced use cases**

### **Code migration assistant**

```
Act√∫a como un migration specialist con experiencia en large-scale modernizations.

MIGRATION TASK: Legacy Java Spring Boot 2.x ‚Üí Spring Boot 3.x + Virtual Threads

CURRENT CODEBASE ANALYSIS:
- 200+ REST endpoints
- Heavy use of @Async methods
- ThreadPool configurations
- Blocking I/O operations
- Custom thread management

MIGRATION STRATEGY NEEDED:
1. **Compatibility Assessment**: What breaks in Spring Boot 3.x?
2. **Virtual Threads Adoption**: Where to apply for maximum benefit?
3. **Performance Impact**: Expected improvements/regressions
4. **Migration Steps**: Detailed phased approach
5. **Risk Mitigation**: Rollback strategies
6. **Testing Strategy**: How to validate the migration

CONSTRAINTS:
- Zero downtime deployment required
- Feature development can't stop
- Team of 12 developers
- 6-week timeline

Provide detailed migration blueprint with code examples for each phase.
```

### **Architecture decision records (ADRs) generator**

```
Generate an ADR for this architectural decision:

DECISION CONTEXT:
We're building a real-time collaborative editing system (like Google Docs) and need to choose the conflict resolution strategy.

OPTIONS CONSIDERED:
1. Operational Transformation (OT)
2. Conflict-free Replicated Data Types (CRDTs)
3. Central locking with optimistic updates

EVALUATION CRITERIA:
- Implementation complexity
- Performance with 100+ concurrent editors
- Offline support requirements
- Consistency guarantees needed

TEAM CONTEXT:
- 6 senior engineers
- 4-month timeline
- Experience with WebRTC, WebSockets
- No prior experience with CRDTs

GENERATE ADR using standard format:
- Title
- Status (proposed/accepted/superseded)
- Context
- Decision
- Consequences (positive/negative)
- Alternatives considered
- Implementation notes
```

## üîß **Tools and automation**

### **Prompt versioning system**

```python
from dataclasses import dataclass
from typing import Dict, List
import hashlib
import json

@dataclass
class PromptVersion:
    version: str
    content: str
    metadata: Dict
    performance_metrics: Dict

class PromptManager:
    def __init__(self):
        self.versions: Dict[str, List[PromptVersion]] = {}

    def save_prompt(self, name: str, content: str, metadata: Dict = None):
        version_hash = hashlib.md5(content.encode()).hexdigest()[:8]

        if name not in self.versions:
            self.versions[name] = []

        prompt_version = PromptVersion(
            version=f"v{len(self.versions[name]) + 1}_{version_hash}",
            content=content,
            metadata=metadata or {},
            performance_metrics={}
        )

        self.versions[name].append(prompt_version)
        return prompt_version.version

    def get_best_prompt(self, name: str) -> PromptVersion:
        if name not in self.versions:
            return None

        # Return version with highest performance score
        return max(
            self.versions[name],
            key=lambda v: v.performance_metrics.get('total_score', 0)
        )
```

### **Prompt testing framework**

```python
class PromptTestCase:
    def __init__(self, name, prompt, expected_criteria):
        self.name = name
        self.prompt = prompt
        self.expected_criteria = expected_criteria

    def run_test(self, ai_client):
        response = ai_client.generate(self.prompt)

        results = {}
        for criterion, validator in self.expected_criteria.items():
            results[criterion] = validator(response)

        return {
            'test_name': self.name,
            'passed': all(results.values()),
            'details': results,
            'response': response
        }

# Ejemplo de uso:
def test_code_generation_completeness(response):
    return all([
        'function' in response or 'def' in response or 'class' in response,
        'test' in response.lower(),
        len(response.split('\n')) > 5
    ])

test_case = PromptTestCase(
    "code_generation_basic",
    "Create a Python function to validate email addresses with tests",
    {
        'completeness': test_code_generation_completeness,
        'has_tests': lambda r: 'test_' in r or 'assert' in r,
        'has_docstring': lambda r: '"""' in r or "'''" in r
    }
)
```

## üöÄ **¬øQu√© sigue?**

Dominas t√©cnicas avanzadas de prompt engineering. Ahora vamos a ver c√≥mo desarrollar aplicaciones completas con LLMs:

**‚û°Ô∏è [Siguiente: Desarrollo con LLMs](./desarrollo-llms.md)**

---

## üìö **Recursos y referencias**

### **Papers y research:**
- "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
- "Constitutional AI: Harmlessness from AI Feedback"
- "Self-Consistency Improves Chain of Thought Reasoning"

### **Tools recomendadas:**
- **PromptPerfect**: Optimizaci√≥n autom√°tica de prompts
- **LangSmith**: Prompt debugging y testing
- **Weights & Biases**: Experiment tracking
- **OpenAI Playground**: Rapid prototyping

### **Prompt libraries:**
- [Awesome ChatGPT Prompts](https://github.com/f/awesome-chatgpt-prompts)
- [LangChain Hub](https://smith.langchain.com/hub)
- [OpenAI Cookbook](https://github.com/openai/openai-cookbook)

---

*El prompt engineering avanzado es parte arte, parte ciencia. Con estas t√©cnicas, puedes resolver problemas de arquitectura compleja y obtener outputs de calidad profesional. En la siguiente secci√≥n aprender√°s a integrar estos prompts en aplicaciones robustas.*