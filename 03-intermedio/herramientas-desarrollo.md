# üß∞ Herramientas de Desarrollo IA

> **El ecosistema completo.** Ya dominas LLMs y MCP. Ahora vamos a las herramientas que potencian el desarrollo: LangChain, Vector databases, RAG y el stack completo para crear aplicaciones de IA robustas y escalables.

## üéØ ¬øQu√© aprender√°s aqu√≠?

- ‚úÖ LangChain: Chains, Agents y Memory
- ‚úÖ Vector databases para b√∫squeda sem√°ntica
- ‚úÖ RAG (Retrieval Augmented Generation) profesional
- ‚úÖ Embeddings y similarity search
- ‚úÖ Architecturas completas de producci√≥n
- ‚úÖ Performance optimization y cost management

## üîó **LangChain: El framework esencial**

### **¬øQu√© es LangChain y por qu√© usarlo?**

**‚ùå Sin framework:**
```python
# C√≥digo repetitivo y complejo
response1 = openai_client.chat.completions.create(...)
parsed1 = parse_response(response1)
response2 = openai_client.chat.completions.create(prompt=f"Based on {parsed1}...")
parsed2 = parse_response(response2)
# ... y as√≠ sucesivamente
```

**‚úÖ Con LangChain:**
```python
# Chain simple y poderoso
from langchain import LLMChain
chain = LLMChain(llm=llm, prompt=prompt)
result = chain.run(input_variables)
```

### **Componentes principales:**

| Componente | Funci√≥n | Ejemplo de uso |
|------------|---------|----------------|
| **LLMs** | Modelos de lenguaje | OpenAI, Anthropic, local models |
| **Prompts** | Templates reutilizables | Few-shot, CoT patterns |
| **Chains** | Secuencias de operaciones | Multi-step reasoning |
| **Agents** | Toma de decisiones | Tool selection, planning |
| **Memory** | Contexto persistente | Conversation history |
| **Retrievers** | B√∫squeda de informaci√≥n | Vector search, web search |

## üîß **LangChain en acci√≥n**

### **1. Basic Chain - Code Analysis**

```python
import os
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.schema import BaseOutputParser
from typing import List, Dict, Any

class CodeAnalysisOutputParser(BaseOutputParser[Dict[str, Any]]):
    """Parse structured code analysis output"""

    def parse(self, text: str) -> Dict[str, Any]:
        try:
            # Extract sections using simple parsing
            sections = {}
            current_section = None
            current_content = []

            for line in text.strip().split('\n'):
                if line.startswith('## '):
                    if current_section:
                        sections[current_section] = '\n'.join(current_content)
                    current_section = line[3:].lower().replace(' ', '_')
                    current_content = []
                else:
                    current_content.append(line)

            if current_section:
                sections[current_section] = '\n'.join(current_content)

            return {
                'issues': self._extract_issues(sections.get('issues', '')),
                'suggestions': self._extract_list(sections.get('suggestions', '')),
                'score': self._extract_score(sections.get('score', ''))
            }

        except Exception as e:
            return {
                'issues': [],
                'suggestions': ['Failed to parse analysis'],
                'score': 0,
                'error': str(e)
            }

    def _extract_issues(self, text: str) -> List[Dict[str, str]]:
        issues = []
        for line in text.split('\n'):
            if line.strip().startswith('-'):
                issues.append({
                    'description': line.strip()[1:].strip(),
                    'severity': 'medium'  # Default severity
                })
        return issues

    def _extract_list(self, text: str) -> List[str]:
        return [line.strip()[1:].strip()
                for line in text.split('\n')
                if line.strip().startswith('-')]

    def _extract_score(self, text: str) -> float:
        import re
        match = re.search(r'(\d+(?:\.\d+)?)', text)
        return float(match.group(1)) if match else 0.0

class CodeAnalyzer:
    def __init__(self, api_key: str):
        self.llm = OpenAI(
            openai_api_key=api_key,
            temperature=0.1,
            max_tokens=2000
        )

        self.analysis_prompt = PromptTemplate(
            input_variables=["code", "language", "focus_areas"],
            template="""
            Analyze this {language} code focusing on {focus_areas}.

            Code to analyze:
            ```{language}
            {code}
            ```

            Provide your analysis in this exact format:

            ## Issues
            - Issue 1 description
            - Issue 2 description

            ## Suggestions
            - Suggestion 1
            - Suggestion 2

            ## Score
            Overall code quality score: X.X/10

            Be specific and actionable in your feedback.
            """
        )

        self.chain = LLMChain(
            llm=self.llm,
            prompt=self.analysis_prompt,
            output_parser=CodeAnalysisOutputParser()
        )

    async def analyze_code(
        self,
        code: str,
        language: str,
        focus_areas: str = "performance, security, maintainability"
    ) -> Dict[str, Any]:
        """Analyze code and return structured results"""

        try:
            result = await self.chain.arun(
                code=code,
                language=language,
                focus_areas=focus_areas
            )

            return {
                'success': True,
                'analysis': result
            }

        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }

    def batch_analyze(self, code_files: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        """Analyze multiple files"""
        results = []

        for file_info in code_files:
            result = self.analyze_code(
                code=file_info['content'],
                language=file_info['language'],
                focus_areas=file_info.get('focus', 'general code quality')
            )

            result['file_path'] = file_info['path']
            results.append(result)

        return results

# Usage example
async def main():
    analyzer = CodeAnalyzer(api_key=os.getenv("OPENAI_API_KEY"))

    python_code = '''
    def get_user(id):
        conn = sqlite3.connect('users.db')
        cursor = conn.cursor()
        cursor.execute(f"SELECT * FROM users WHERE id = {id}")
        return cursor.fetchone()
    '''

    result = await analyzer.analyze_code(
        code=python_code,
        language="python",
        focus_areas="security, best practices"
    )

    if result['success']:
        analysis = result['analysis']
        print(f"Code Quality Score: {analysis['score']}/10")

        print("\nIssues Found:")
        for issue in analysis['issues']:
            print(f"- {issue['description']}")

        print("\nSuggestions:")
        for suggestion in analysis['suggestions']:
            print(f"- {suggestion}")
    else:
        print(f"Analysis failed: {result['error']}")
```

### **2. Agents - Autonomous problem solving**

```python
from langchain.agents import initialize_agent, AgentType, Tool
from langchain.tools import BaseTool
from langchain.llms import OpenAI
from langchain.memory import ConversationBufferWindowMemory
import requests
import json
from typing import Optional

class GitHubSearchTool(BaseTool):
    name = "github_search"
    description = "Search GitHub repositories and issues for code examples and solutions"

    def _run(self, query: str) -> str:
        """Search GitHub for relevant information"""
        try:
            # This is a simplified example - you'd use GitHub API
            headers = {
                'Accept': 'application/vnd.github.v3+json',
                'Authorization': f'token {os.getenv("GITHUB_TOKEN")}'
            }

            search_url = f"https://api.github.com/search/repositories?q={query}&sort=stars"
            response = requests.get(search_url, headers=headers)

            if response.status_code == 200:
                data = response.json()
                repos = data.get('items', [])[:3]  # Top 3 results

                results = []
                for repo in repos:
                    results.append(f"- {repo['name']}: {repo['description']} "
                                 f"({repo['stargazers_count']} stars)")

                return "Top GitHub repositories:\n" + "\n".join(results)
            else:
                return f"GitHub search failed: {response.status_code}"

        except Exception as e:
            return f"Error searching GitHub: {str(e)}"

    async def _arun(self, query: str) -> str:
        return self._run(query)

class StackOverflowTool(BaseTool):
    name = "stackoverflow_search"
    description = "Search StackOverflow for programming questions and answers"

    def _run(self, query: str) -> str:
        """Search StackOverflow API"""
        try:
            api_url = "https://api.stackexchange.com/2.3/search"
            params = {
                'order': 'desc',
                'sort': 'votes',
                'intitle': query,
                'site': 'stackoverflow'
            }

            response = requests.get(api_url, params=params)

            if response.status_code == 200:
                data = response.json()
                questions = data.get('items', [])[:3]

                results = []
                for q in questions:
                    results.append(f"- {q['title']} "
                                 f"(Score: {q['score']}, "
                                 f"Answers: {q['answer_count']})")

                return "Top StackOverflow questions:\n" + "\n".join(results)
            else:
                return f"StackOverflow search failed: {response.status_code}"

        except Exception as e:
            return f"Error searching StackOverflow: {str(e)}"

    async def _arun(self, query: str) -> str:
        return self._run(query)

class DocumentationTool(BaseTool):
    name = "documentation_lookup"
    description = "Look up official documentation for programming languages and frameworks"

    def _run(self, query: str) -> str:
        """Lookup documentation (simplified)"""
        # This would integrate with documentation APIs
        doc_sources = {
            'python': 'https://docs.python.org/3/',
            'javascript': 'https://developer.mozilla.org/',
            'react': 'https://reactjs.org/docs/',
            'django': 'https://docs.djangoproject.com/',
            'flutter': 'https://flutter.dev/docs'
        }

        for tech, url in doc_sources.items():
            if tech in query.lower():
                return f"Check the official {tech.title()} documentation: {url}"

        return "For official documentation, check the technology's main website"

    async def _arun(self, query: str) -> str:
        return self._run(query)

class DevelopmentAssistant:
    def __init__(self, api_key: str):
        self.llm = OpenAI(
            openai_api_key=api_key,
            temperature=0.1
        )

        # Initialize memory for conversation context
        self.memory = ConversationBufferWindowMemory(
            memory_key="chat_history",
            k=10  # Remember last 10 interactions
        )

        # Define available tools
        self.tools = [
            GitHubSearchTool(),
            StackOverflowTool(),
            DocumentationTool(),
        ]

        # Initialize agent
        self.agent = initialize_agent(
            tools=self.tools,
            llm=self.llm,
            agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,
            memory=self.memory,
            verbose=True,  # Set to False in production
            max_iterations=3,
            early_stopping_method="generate"
        )

    async def solve_problem(self, problem_description: str) -> str:
        """Use agent to solve development problems"""

        # Enhanced prompt for better tool usage
        enhanced_prompt = f"""
        Help me solve this development problem: {problem_description}

        Use the available tools to:
        1. Search for existing solutions on GitHub
        2. Find relevant StackOverflow discussions
        3. Check official documentation if needed

        Then provide a comprehensive solution with:
        - Step-by-step approach
        - Code examples when applicable
        - Best practices recommendations
        - Potential pitfalls to avoid
        """

        try:
            result = await self.agent.arun(enhanced_prompt)
            return result
        except Exception as e:
            return f"Error solving problem: {str(e)}"

    def get_conversation_history(self) -> str:
        """Get chat history for context"""
        return self.memory.buffer

# Usage example
async def main():
    assistant = DevelopmentAssistant(api_key=os.getenv("OPENAI_API_KEY"))

    problems = [
        "How do I implement real-time WebSocket connections in Flutter?",
        "What's the best way to handle database migrations in Django?",
        "How can I optimize React component re-rendering performance?"
    ]

    for problem in problems:
        print(f"\nüîç Problem: {problem}")
        print("=" * 80)

        solution = await assistant.solve_problem(problem)
        print(f"üí° Solution:\n{solution}\n")
```

## üóÑÔ∏è **Vector Databases**

### **¬øPor qu√© Vector Databases?**

```python
# Traditional search (keyword-based)
query = "user authentication"
# Finds: exact matches of "user" and "authentication"

# Semantic search (vector-based)
query_embedding = model.encode("user authentication")
# Finds: login systems, OAuth, JWT, security, identity management
```

### **Popular Vector Databases:**

| Database | Best for | Pros | Cons |
|----------|----------|------|------|
| **Pinecone** | Production apps | Managed, scalable | Paid service |
| **Weaviate** | Self-hosted | Open source, GraphQL | Complex setup |
| **Chroma** | Development | Simple, fast | Limited scale |
| **Qdrant** | High performance | Fast, Rust-based | Newer ecosystem |

### **Vector Database Implementation - Chroma**

```python
import chromadb
from chromadb.config import Settings
import openai
from typing import List, Dict, Any
import uuid
import json

class SemanticCodeSearch:
    def __init__(self, openai_api_key: str, persist_directory: str = "./chroma_db"):
        self.openai_client = openai.OpenAI(api_key=openai_api_key)

        # Initialize Chroma with persistence
        self.chroma_client = chromadb.PersistentClient(path=persist_directory)

        # Create or get collection
        self.collection = self.chroma_client.get_or_create_collection(
            name="code_snippets",
            metadata={"description": "Code snippets with semantic search"}
        )

    def get_embedding(self, text: str) -> List[float]:
        """Get embedding using OpenAI's text-embedding-ada-002"""
        try:
            response = self.openai_client.embeddings.create(
                model="text-embedding-ada-002",
                input=text
            )
            return response.data[0].embedding
        except Exception as e:
            raise Exception(f"Failed to get embedding: {str(e)}")

    def add_code_snippet(
        self,
        code: str,
        language: str,
        description: str,
        tags: List[str] = None,
        file_path: str = None
    ) -> str:
        """Add a code snippet to the vector database"""

        # Create searchable text combining code and description
        searchable_text = f"{description}\n\nCode:\n{code}"

        # Generate embedding
        embedding = self.get_embedding(searchable_text)

        # Create unique ID
        snippet_id = str(uuid.uuid4())

        # Metadata
        metadata = {
            "language": language,
            "description": description,
            "tags": json.dumps(tags or []),
            "file_path": file_path or "",
            "code_length": len(code)
        }

        # Add to collection
        self.collection.add(
            embeddings=[embedding],
            documents=[code],
            metadatas=[metadata],
            ids=[snippet_id]
        )

        return snippet_id

    def search_similar_code(
        self,
        query: str,
        language_filter: str = None,
        n_results: int = 5
    ) -> List[Dict[str, Any]]:
        """Search for similar code snippets"""

        # Generate query embedding
        query_embedding = self.get_embedding(query)

        # Prepare where clause for filtering
        where_clause = {}
        if language_filter:
            where_clause["language"] = language_filter

        # Search in vector database
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results,
            where=where_clause if where_clause else None,
            include=["documents", "metadatas", "distances"]
        )

        # Format results
        formatted_results = []

        for i, doc in enumerate(results['documents'][0]):
            metadata = results['metadatas'][0][i]
            distance = results['distances'][0][i]

            formatted_results.append({
                'code': doc,
                'language': metadata['language'],
                'description': metadata['description'],
                'tags': json.loads(metadata.get('tags', '[]')),
                'file_path': metadata.get('file_path', ''),
                'similarity_score': 1 - distance,  # Convert distance to similarity
                'metadata': metadata
            })

        return formatted_results

    def add_codebase(self, codebase_path: str):
        """Add entire codebase to vector database"""
        import os
        import ast

        supported_extensions = {'.py', '.js', '.ts', '.jsx', '.tsx', '.java', '.cpp', '.c'}
        language_map = {
            '.py': 'python',
            '.js': 'javascript',
            '.ts': 'typescript',
            '.jsx': 'javascript',
            '.tsx': 'typescript',
            '.java': 'java',
            '.cpp': 'cpp',
            '.c': 'c'
        }

        for root, dirs, files in os.walk(codebase_path):
            for file in files:
                file_path = os.path.join(root, file)
                file_ext = os.path.splitext(file)[1]

                if file_ext in supported_extensions:
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()

                        # Extract functions/classes (simplified)
                        functions = self._extract_functions(content, language_map[file_ext])

                        for func in functions:
                            self.add_code_snippet(
                                code=func['code'],
                                language=language_map[file_ext],
                                description=func['description'],
                                tags=func['tags'],
                                file_path=file_path
                            )

                    except Exception as e:
                        print(f"Failed to process {file_path}: {str(e)}")

    def _extract_functions(self, code: str, language: str) -> List[Dict[str, Any]]:
        """Extract functions from code (simplified Python example)"""
        functions = []

        if language == 'python':
            try:
                tree = ast.parse(code)

                for node in ast.walk(tree):
                    if isinstance(node, ast.FunctionDef):
                        # Get function code
                        func_lines = code.split('\n')[node.lineno-1:node.end_lineno]
                        func_code = '\n'.join(func_lines)

                        # Extract docstring for description
                        description = ast.get_docstring(node) or f"Function: {node.name}"

                        # Generate tags based on function name and content
                        tags = self._generate_tags(node.name, func_code)

                        functions.append({
                            'code': func_code,
                            'description': description,
                            'tags': tags
                        })

            except SyntaxError:
                # If parsing fails, treat whole file as one snippet
                functions.append({
                    'code': code,
                    'description': f"Code file",
                    'tags': []
                })

        else:
            # For other languages, add as whole file for now
            functions.append({
                'code': code,
                'description': f"{language.title()} code",
                'tags': []
            })

        return functions

    def _generate_tags(self, function_name: str, code: str) -> List[str]:
        """Generate tags based on function name and code content"""
        tags = []

        # Common patterns
        if 'test' in function_name.lower():
            tags.append('test')
        if any(word in function_name.lower() for word in ['get', 'fetch', 'retrieve']):
            tags.append('getter')
        if any(word in function_name.lower() for word in ['set', 'update', 'modify']):
            tags.append('setter')
        if 'api' in code.lower():
            tags.append('api')
        if 'database' in code.lower() or 'sql' in code.lower():
            tags.append('database')

        return tags

    def get_collection_stats(self) -> Dict[str, Any]:
        """Get statistics about the collection"""
        count = self.collection.count()

        # Get all metadata to analyze
        all_data = self.collection.get(include=["metadatas"])

        languages = {}
        total_code_length = 0

        for metadata in all_data['metadatas']:
            lang = metadata['language']
            languages[lang] = languages.get(lang, 0) + 1
            total_code_length += metadata.get('code_length', 0)

        return {
            'total_snippets': count,
            'languages': languages,
            'average_code_length': total_code_length / count if count > 0 else 0
        }

# Usage example
async def main():
    search_engine = SemanticCodeSearch(
        openai_api_key=os.getenv("OPENAI_API_KEY"),
        persist_directory="./code_search_db"
    )

    # Add some code snippets
    python_auth = '''
def authenticate_user(username, password):
    """Authenticate user with username and password"""
    user = User.objects.filter(username=username).first()
    if user and check_password(password, user.password_hash):
        return generate_jwt_token(user.id)
    return None
    '''

    search_engine.add_code_snippet(
        code=python_auth,
        language="python",
        description="User authentication with JWT token generation",
        tags=["authentication", "jwt", "security", "login"]
    )

    # Search for similar code
    query = "how to verify user credentials and create session"
    results = search_engine.search_similar_code(query, n_results=3)

    for i, result in enumerate(results):
        print(f"\n--- Result {i+1} (Similarity: {result['similarity_score']:.3f}) ---")
        print(f"Language: {result['language']}")
        print(f"Description: {result['description']}")
        print(f"Tags: {', '.join(result['tags'])}")
        print("Code:")
        print(result['code'])
```

## üîç **RAG (Retrieval Augmented Generation)**

### **RAG Architecture:**

```mermaid
graph TD
    A[User Query] --> B[Query Processing]
    B --> C[Vector Search]
    C --> D[Document Retrieval]
    D --> E[Context Assembly]
    E --> F[LLM Generation]
    F --> G[Final Response]
```

### **Professional RAG Implementation:**

```python
from langchain.vectorstores import Chroma
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
import os
from typing import List, Dict, Any, Optional

class ProductionRAGSystem:
    def __init__(
        self,
        openai_api_key: str,
        vector_store_path: str = "./rag_vectorstore",
        chunk_size: int = 1000,
        chunk_overlap: int = 200
    ):
        self.openai_api_key = openai_api_key

        # Initialize embeddings
        self.embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)

        # Initialize text splitter
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", " ", ""]
        )

        # Initialize LLM
        self.llm = OpenAI(
            openai_api_key=openai_api_key,
            temperature=0.1,
            max_tokens=2000
        )

        # Initialize vector store
        self.vector_store = Chroma(
            persist_directory=vector_store_path,
            embedding_function=self.embeddings
        )

        # Custom RAG prompt
        self.rag_prompt = PromptTemplate(
            template="""
            You are a helpful assistant that answers questions based on provided context.
            Use the context below to answer the question. If the context doesn't contain
            enough information to answer the question, say so clearly.

            Context:
            {context}

            Question: {question}

            Instructions:
            - Base your answer primarily on the provided context
            - If information is missing, acknowledge it
            - Provide specific examples from the context when applicable
            - If the context seems irrelevant to the question, say so

            Answer:
            """,
            input_variables=["context", "question"]
        )

        # Initialize retrieval chain
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vector_store.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 6}
            ),
            chain_type_kwargs={"prompt": self.rag_prompt},
            return_source_documents=True
        )

    def ingest_documents(self, documents_path: str, file_extensions: List[str] = None):
        """Ingest documents from a directory"""

        if file_extensions is None:
            file_extensions = ['.txt', '.md', '.py', '.js', '.ts', '.jsx', '.tsx']

        documents = []

        for ext in file_extensions:
            loader = DirectoryLoader(
                documents_path,
                glob=f"**/*{ext}",
                loader_cls=TextLoader,
                loader_kwargs={'encoding': 'utf-8'}
            )
            docs = loader.load()
            documents.extend(docs)

        if not documents:
            raise ValueError(f"No documents found in {documents_path}")

        # Split documents into chunks
        chunks = self.text_splitter.split_documents(documents)

        print(f"Processing {len(documents)} documents into {len(chunks)} chunks")

        # Add to vector store in batches
        batch_size = 100
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i + batch_size]
            self.vector_store.add_documents(batch)
            print(f"Processed batch {i//batch_size + 1}/{(len(chunks)-1)//batch_size + 1}")

        # Persist the vector store
        self.vector_store.persist()

        return len(chunks)

    def query(
        self,
        question: str,
        include_sources: bool = True,
        search_kwargs: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Query the RAG system"""

        if search_kwargs:
            # Update retriever with custom search parameters
            self.qa_chain.retriever.search_kwargs.update(search_kwargs)

        try:
            result = self.qa_chain({"query": question})

            response = {
                "answer": result["result"],
                "question": question
            }

            if include_sources and "source_documents" in result:
                sources = []
                for doc in result["source_documents"]:
                    sources.append({
                        "content": doc.page_content[:200] + "...",
                        "metadata": doc.metadata,
                        "source": doc.metadata.get('source', 'Unknown')
                    })
                response["sources"] = sources

            return response

        except Exception as e:
            return {
                "answer": f"Error processing query: {str(e)}",
                "question": question,
                "error": True
            }

    def batch_query(self, questions: List[str]) -> List[Dict[str, Any]]:
        """Process multiple questions"""
        results = []

        for question in questions:
            print(f"Processing: {question[:50]}...")
            result = self.query(question)
            results.append(result)

        return results

    def search_similar_documents(
        self,
        query: str,
        k: int = 5
    ) -> List[Dict[str, Any]]:
        """Search for similar documents without generating answer"""

        similar_docs = self.vector_store.similarity_search_with_score(query, k=k)

        results = []
        for doc, score in similar_docs:
            results.append({
                "content": doc.page_content,
                "metadata": doc.metadata,
                "similarity_score": score
            })

        return results

    def get_collection_info(self) -> Dict[str, Any]:
        """Get information about the vector store collection"""

        try:
            # Get a sample to analyze
            sample_docs = self.vector_store.similarity_search("sample", k=10)

            file_types = {}
            total_docs = len(sample_docs)

            for doc in sample_docs:
                source = doc.metadata.get('source', 'unknown')
                ext = os.path.splitext(source)[1] or 'no_extension'
                file_types[ext] = file_types.get(ext, 0) + 1

            return {
                "sample_size": total_docs,
                "file_types": file_types,
                "vector_store_path": self.vector_store._persist_directory
            }

        except Exception as e:
            return {"error": str(e)}

    def update_documents(self, documents_path: str):
        """Update the vector store with new documents"""
        # This is a simplified update - in production you'd want to track changes
        return self.ingest_documents(documents_path)

# Advanced RAG with re-ranking
class AdvancedRAGSystem(ProductionRAGSystem):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        # Initialize re-ranker (using sentence-transformers)
        try:
            from sentence_transformers import CrossEncoder
            self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
        except ImportError:
            print("Warning: sentence-transformers not installed. Re-ranking disabled.")
            self.reranker = None

    def query_with_reranking(
        self,
        question: str,
        initial_k: int = 20,
        final_k: int = 6
    ) -> Dict[str, Any]:
        """Query with re-ranking for better relevance"""

        if not self.reranker:
            return self.query(question)

        # Get more documents initially
        similar_docs = self.vector_store.similarity_search(question, k=initial_k)

        # Re-rank documents
        pairs = [[question, doc.page_content] for doc in similar_docs]
        scores = self.reranker.predict(pairs)

        # Sort by re-ranking scores and take top k
        doc_score_pairs = list(zip(similar_docs, scores))
        doc_score_pairs.sort(key=lambda x: x[1], reverse=True)

        top_docs = [doc for doc, _ in doc_score_pairs[:final_k]]

        # Create context from top documents
        context = "\n\n".join([doc.page_content for doc in top_docs])

        # Generate answer using the custom prompt
        prompt = self.rag_prompt.format(context=context, question=question)
        answer = self.llm(prompt)

        return {
            "answer": answer,
            "question": question,
            "sources": [
                {
                    "content": doc.page_content[:200] + "...",
                    "metadata": doc.metadata,
                    "rerank_score": float(score)
                }
                for doc, score in doc_score_pairs[:final_k]
            ]
        }

# Usage example
async def main():
    rag_system = AdvancedRAGSystem(
        openai_api_key=os.getenv("OPENAI_API_KEY"),
        vector_store_path="./advanced_rag_db"
    )

    # Ingest your codebase or documentation
    print("Ingesting documents...")
    num_chunks = rag_system.ingest_documents(
        "./your_project_docs",
        file_extensions=['.md', '.py', '.js', '.ts']
    )
    print(f"Ingested {num_chunks} document chunks")

    # Test queries
    questions = [
        "How do I implement authentication in this codebase?",
        "What are the main API endpoints available?",
        "How is error handling implemented?",
        "What testing frameworks are used?"
    ]

    for question in questions:
        print(f"\nüîç Question: {question}")
        print("-" * 80)

        result = rag_system.query_with_reranking(question)

        print(f"üìù Answer: {result['answer']}\n")

        if 'sources' in result:
            print("üìö Sources:")
            for i, source in enumerate(result['sources'][:3]):
                print(f"{i+1}. {source['metadata'].get('source', 'Unknown')} "
                      f"(Score: {source.get('rerank_score', 'N/A'):.3f})")
                print(f"   {source['content'][:100]}...\n")
```

## üöÄ **¬øQu√© sigue?**

Dominas las herramientas fundamentales del ecosistema IA. Vamos al siguiente nivel: desarrollo avanzado con arquitecturas complejas:

**‚û°Ô∏è [Siguiente: Workflows de Desarrollo con IA](../04-avanzado/workflows-desarrollo.md)**

---

## üìä **Performance Benchmarks**

### **Vector Database Comparison:**

| Database | Ingestion Speed | Query Speed | Memory Usage | Scalability |
|----------|-----------------|-------------|--------------|-------------|
| **Chroma** | Fast | Good | Low | Medium |
| **Pinecone** | Medium | Excellent | N/A (hosted) | High |
| **Weaviate** | Medium | Good | Medium | High |
| **Qdrant** | Fast | Excellent | Low | High |

### **RAG Performance Metrics:**

| Metric | Without RAG | Basic RAG | Advanced RAG |
|--------|-------------|-----------|--------------|
| **Answer Accuracy** | 60% | 85% | 92% |
| **Source Attribution** | 0% | 70% | 90% |
| **Response Time** | 500ms | 1.2s | 2.1s |
| **Context Relevance** | N/A | 75% | 88% |

## üí∞ **Cost Optimization Tips**

### **Embedding Costs:**
- Use `text-embedding-ada-002` for cost-effectiveness
- Cache embeddings for repeated content
- Batch embedding requests when possible

### **Vector Storage:**
- Self-hosted solutions (Chroma, Qdrant) for cost control
- Managed solutions (Pinecone) for scale and reliability
- Consider hybrid approaches

### **LLM Costs in RAG:**
- Optimize chunk sizes to reduce token usage
- Use cheaper models for re-ranking
- Implement response caching for common queries

---

*Las herramientas de desarrollo IA te permiten crear aplicaciones sofisticadas que van mucho m√°s all√° de simples chatbots. Con LangChain, vector databases y RAG, puedes construir sistemas que entienden y act√∫an sobre informaci√≥n compleja y espec√≠fica de tu dominio.*